{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4E0mVFNsZQo",
        "outputId": "3815f626-a27f-4110-d06e-00647a31c032"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.3.3)\n",
            "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.25.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Requirement already satisfied: httpcore in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore->httpx<1,>=0.23.0->openai) (0.14.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rc0zQ_qCsaaM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98Ns3W9gstWv",
        "outputId": "2447c0a3-3c38-43cf-8a71-d23ed281c1a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "import re\n",
        "import ast\n",
        "from google.colab import drive\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "client = OpenAI(\n",
        "    api_key=[API_KEY],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7iD7fLAsob3"
      },
      "outputs": [],
      "source": [
        "#df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/forensik/DS/fix/email.csv')\n",
        "#df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/forensik/DS/fix/casperrgen.csv')\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/forensik/DS/CANON (1).csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOMbrpVrs7xf"
      },
      "outputs": [],
      "source": [
        "nama_column = 'message'\n",
        "df[nama_column] = df[nama_column].str.replace(r'[:/$_]', ' ', regex=True) #prepocessing mengapus beberapa karakter didalam dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03bmZ7WSs9aP",
        "outputId": "902668a5-0b89-49e1-ee19-93e263c73f94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0      TSK   FAT1 Owner identifier  0 Group identifie...\n",
            "1      TSK   FAT1 Owner identifier  0 Group identifie...\n",
            "2      TSK   FAT1 Owner identifier  0 Group identifie...\n",
            "3      TSK   FAT2 Owner identifier  0 Group identifie...\n",
            "4      TSK   FAT2 Owner identifier  0 Group identifie...\n",
            "                             ...                        \n",
            "110    TSK  DCIM 100CANON IMG 0049.JPG Type  file Own...\n",
            "111    TSK  DCIM 100CANON IMG 0050.JPG Type  file Own...\n",
            "112    TSK  DCIM 100CANON IMG 0050.JPG Type  file Own...\n",
            "113    TSK  DCIM 100CANON IMG 0051.JPG Type  file Own...\n",
            "114    TSK  DCIM 100CANON IMG 0051.JPG Type  file Own...\n",
            "Name: message, Length: 115, dtype: object\n"
          ]
        }
      ],
      "source": [
        "#lihat data yang sudah di prepocessing\n",
        "data_clumn = df[nama_column]\n",
        "print(data_clumn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oV48NFajs-rl"
      },
      "outputs": [],
      "source": [
        "#load dataset per row\n",
        "start_index = 1\n",
        "end_index = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjbX6StBtAB-"
      },
      "outputs": [],
      "source": [
        "def openai_chat_completion_response(final_prompt):\n",
        "  response = client.chat.completions.create(\n",
        "              model=\"gpt-3.5-turbo\",\n",
        "              max_tokens=100,\n",
        "              messages=[\n",
        "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                    {\"role\": \"user\", \"content\": USER_PROMPT_1},\n",
        "                    {\"role\": \"assistant\", \"content\": ASSISTANT_PROMPT_1},\n",
        "                    {\"role\": \"user\", \"content\": final_prompt}\n",
        "                ]\n",
        "            )\n",
        "\n",
        "  return response.choices[0].message.content.strip(\" \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = \"You are a smart and intelligent Named Entity Recognition (NER) system. I will provide you the definition of the entities you need to extract, the sentence from where your extract the entities and the output format with examples.\"\n",
        "\n",
        "USER_PROMPT_1 = \"Are you clear about your role?\"\n",
        "\n",
        "ASSISTANT_PROMPT_1 = \"Sure, I'm ready to help you with your NER task. Please provide me with the necessary information to get started.\"\n",
        "\n",
        "GUIDELINES_PROMPT_3= (\n",
        " \"i have a log file obtained from digital corpora. i want to perform Named Entity Recognation to extract some of the entites based on log messages, Please extract the entites from log messages for forensic investigation purpose Example of log messages: \"\n",
        ")"
      ],
      "metadata": {
        "id": "1mvDHMCH2QhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index in range(start_index, min(end_index, len(df))):\n",
        "    message = df.loc[index, 'message']\n",
        "\n",
        "    current_prompt = f\"i have a log messages contained from email. I want to perform Named Entity Recognation to extract some of the entities mentioned. Please show identify and extract relevant entities from the log '{message}' for forensic investigation. Show spesific entites based on forensic investigation fields on a JSON format without description about the entity\"\n",
        "\n",
        "    ners = openai_chat_completion_response(current_prompt)\n",
        "\n",
        "    print(f\"Sentence: {message}\\n\\nOutput: {ners} \\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sH2N394xWW0",
        "outputId": "3e979a72-36e1-4412-ee70-339b6c55411e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: Title   Subject   Author  Simson Garfinkel Keywords   Template  Normal.dotm Revision number  1 Last saved by  Simson Garfinkel Number of pages  1 Number of words  0 Number of characters  0 Application  Microsoft Macintosh Word Security flags  0x00000000 []\n",
            "\n",
            "Output: Based on your input log message, here are the extracted entities relevant to forensic investigation:\n",
            "\n",
            "JSON Format:\n",
            "```\n",
            "{\n",
            "  \"Title\": \"Microsoft Macintosh Word\",\n",
            "  \"Subject\": \"\",\n",
            "  \"Author\": \"Simson Garfinkel\",\n",
            "  \"Keywords\": \"\",\n",
            "  \"Template\": \"Normal.dotm\",\n",
            "  \"Revision number\": \"1\",\n",
            "  \"Last saved by\": \"Simson Garfinkel\",\n",
            "  \"Number of pages\": \"1\",\n",
            "  \"Number of words\": \" \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROMPT CASPER-GEN"
      ],
      "metadata": {
        "id": "Jc0Fvpstx2aZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for index in range(start_index, min(end_index, len(df))):\n",
        "    message = df.loc[index, 'message']\n",
        "\n",
        "    current_prompt = f\"i have a log messages contained from ext3 system log file . I want to perform Named Entity Recognation to extract some of the entities mentioned. Please show identify and extract relevant entities from the log '{message}' for forensic investigation. Show spesific entites based on forensic investigation fields on a JSON format without description about the entity\"\n",
        "\n",
        "    ners = openai_chat_completion_response(current_prompt)\n",
        "\n",
        "    print(f\"Sentence: {message}\\n\\nOutput: {ners} \\n\\n\")"
      ],
      "metadata": {
        "id": "yW0F_bwhxk9K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13d68927-5e6b-4410-dd37-75134a8891ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: Title  CIP Day Keynote Address Author   Template  C \\Program Files\\Microsoft Office\\Templates\\Presentations\\Project Overview (Standard).pot Revision number  187 Last saved by  Booz Allen User Number of words  606 Application  Microsoft PowerPoint 7.0\n",
            "\n",
            "Output: Sure! Based on the given log message, I can identify the following relevant entities related to forensic investigation:\n",
            "\n",
            "1. Title: \"CIP Day Keynote Address\"\n",
            "2. Author: \"Template\"\n",
            "3. FilePath: \"C \\Program Files\\Microsoft Office\\Templates\\Presentations\\Project Overview (Standard).pot\"\n",
            "4. RevisionNumber: 187\n",
            "5. LastSavedBy: \"Booz Allen User\"\n",
            "6. NumberOfWords: 606\n",
            "7. Application: \"Microsoft PowerPoint \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROMPOT CAMERA CANON"
      ],
      "metadata": {
        "id": "lSsdit8cx5Sj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CYkqFqytCfQ",
        "outputId": "919c63ae-a756-43c4-b6c8-305c4b42d9e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: TSK   FAT1 Owner identifier  0 Group identifier  0 Mode  0o000 Number of links  1\n",
            "\n",
            "Output: Based on the log message you provided, here are the relevant entities extracted for forensic investigation fields in JSON format:\n",
            "\n",
            "{\n",
            "  \"Owner Identifier\": \"0\",\n",
            "  \"Group Identifier\": \"0\",\n",
            "  \"Mode\": \"0o000\",\n",
            "  \"Number of Links\": \"1\"\n",
            "}\n",
            "\n",
            "Note that the entities are extracted based on their field names in forensic investigation and no entity description is included in this format. \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for index in range(start_index, min(end_index, len(df))):\n",
        "    message = df.loc[index, 'message']\n",
        "\n",
        "    current_prompt = f\"i have a log messages contained from canon camera. I want to perform Named Entity Recognation to extract some of the entities mentioned. Please show identify and extract relevant entities from the log '{message}' for forensic investigation. Show spesific entites based on forensic investigation fields on a JSON format without description about the entity\"\n",
        "\n",
        "    ners = openai_chat_completion_response(current_prompt)\n",
        "\n",
        "    print(f\"Sentence: {message}\\n\\nOutput: {ners} \\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLAUDE"
      ],
      "metadata": {
        "id": "G6Aj-FG2EWfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install anthropic\n",
        "from anthropic import Anthropic, HUMAN_PROMPT, AI_PROMPT"
      ],
      "metadata": {
        "id": "_TwhKovgEWHn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15d18dc8-ad58-4fc7-b12b-73111037901e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting anthropic\n",
            "  Downloading anthropic-0.7.2-py3-none-any.whl (807 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m807.6/807.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from anthropic) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (0.25.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (1.10.13)\n",
            "Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (0.15.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from anthropic) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->anthropic) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->anthropic) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->anthropic) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic) (2023.7.22)\n",
            "Requirement already satisfied: httpcore in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic) (1.0.2)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.0->anthropic) (0.19.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (6.0.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (23.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore->httpx<1,>=0.23.0->anthropic) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.0.7)\n",
            "Installing collected packages: anthropic\n",
            "Successfully installed anthropic-0.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nu3G7XCbtKi8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "238ae3d1-9a08-4aad-8d69-d4262b4f82da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: Title   Subject   Author  Simson Garfinkel Keywords   Template  Normal.dotm Revision number  1 Last saved by  Simson Garfinkel Number of pages  1 Number of words  0 Number of characters  0 Application  Microsoft Macintosh Word Security flags  0x00000000 []\n",
            "\n",
            "\n",
            " Here is the JSON output with the named entities extracted from the log message:\n",
            "\n",
            "{\n",
            "\"Title\": \"Title\",  \n",
            "\"Subject\": \"Subject\",\n",
            "\"Author\": \"Simson Garfinkel\",\n",
            "\"Keywords\": \"Keywords\",  \n",
            "\"Template\": \"Normal.dotm\",\n",
            "\"Revision number\": \"1\",\n",
            "\"Last saved by\": \"Simson Garfinkel\", \n",
            "\"Number of pages\": \"1\",\n",
            "\"Number of words\": \"0\",\n",
            "\"Number of characters\": \"0\",  \n",
            "\"Application\": \"Microsoft Macintosh Word\",\n",
            "\"Security flags\": \"0x00000000 []\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "anthropic = Anthropic(\n",
        "    api_key=[API_KEY]\n",
        ")\n",
        "\n",
        "for index in range(start_index, min(end_index, len(df))):\n",
        "    message = df.loc[index, 'message']\n",
        "\n",
        "\n",
        "    #GUIDELINES_PROMPT_CLAUDE_1 (\n",
        "    prompt1= f\"i have a log messages contained from emails. I want to perform Named Entity Recognation to extract some of the entities mentioned. Please show identify and extract relevant entities from the log '{message}' for forensic investigation. Show spesific entites based on forensic investigation fields on a JSON format without description about the entity\"\n",
        "\n",
        "    completion = anthropic.completions.create(\n",
        "        model=\"claude-1\",\n",
        "        max_tokens_to_sample=300,\n",
        "        prompt=f\"{HUMAN_PROMPT}{prompt1}{AI_PROMPT}\",\n",
        "    )\n",
        "\n",
        "    print(f\"Sentence: {message}\\n\\n\")\n",
        "    print(completion.completion)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CASPER"
      ],
      "metadata": {
        "id": "GWvmt-AiBDf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "anthropic = Anthropic(\n",
        "    api_key=z{API_KEY},\n",
        ")\n",
        "\n",
        "for index in range(start_index, min(end_index, len(df))):\n",
        "    message = df.loc[index, 'message']\n",
        "\n",
        "\n",
        "    #GUIDELINES_PROMPT_CLAUDE_1 (\n",
        "    prompt1= f\"i have a log messages contained from Flash USB disk. I want to perform Named Entity Recognation to extract some of the entities mentioned. Please show identify and extract relevant entities from the log '{message}' for forensic investigation. Show spesific entites based on forensic investigation fields on a JSON format without description about the entity\"\n",
        "\n",
        "    completion = anthropic.completions.create(\n",
        "        model=\"claude-1\",\n",
        "        max_tokens_to_sample=300,\n",
        "        prompt=f\"{HUMAN_PROMPT}{prompt1}{AI_PROMPT}\",\n",
        "    )\n",
        "\n",
        "    print(f\"Sentence: {message}\\n\\n\")\n",
        "    print(completion.completion)"
      ],
      "metadata": {
        "id": "T7I_uf0okztL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "496b18bf-524f-4ee0-ec18-1ab178da0e57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: Title  CIP Day Keynote Address Author   Template  C \\Program Files\\Microsoft Office\\Templates\\Presentations\\Project Overview (Standard).pot Revision number  187 Last saved by  Booz Allen User Number of words  606 Application  Microsoft PowerPoint 7.0\n",
            "\n",
            "\n",
            " Here is the log message with named entities extracted in JSON format:\n",
            "\n",
            "{\n",
            "\"Title\":\"CIP Day Keynote Address\",\n",
            "\"Author\":\"Template\",  \n",
            "\"Revision_number\":\"187\",\n",
            "\"Last_saved_by\":\"Booz Allen User\",\n",
            "\"Number_of_words\":\"606\",  \n",
            "\"Application\":\"Microsoft PowerPoint 7.0\",\n",
            "\"C\":\"\\Program Files\\Microsoft Office\\Templates\\Presentations\\Project Overview (Standard).pot\" \n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CANNON"
      ],
      "metadata": {
        "id": "QVCQWPfIBGLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "anthropic = Anthropic(\n",
        "    api_key= [API_KEY]\n",
        ")\n",
        "\n",
        "for index in range(start_index, min(end_index, len(df))):\n",
        "    message = df.loc[index, 'message']\n",
        "\n",
        "\n",
        "    #GUIDELINES_PROMPT_CLAUDE_1 (\n",
        "    prompt1= f\"i have a log messages obtainded from a Cannon Camera. I want to perform Named Entity Recognation to extract some of the entities mentioned. Please show identify and extract relevant entities from the log '{message}' for forensic investigation. Show spesific entites based on forensic investigation fields on a JSON format without description about the entity.\"\n",
        "\n",
        "    completion = anthropic.completions.create(\n",
        "        model=\"claude-1\",\n",
        "        max_tokens_to_sample=300,\n",
        "        prompt=f\"{HUMAN_PROMPT}{prompt1}{AI_PROMPT}\",\n",
        "    )\n",
        "\n",
        "    print(f\"Sentence: {message}\\n\\n\")\n",
        "    print(completion.completion)"
      ],
      "metadata": {
        "id": "-edegEnPA-_H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "962998ee-944b-42c2-c1a8-abd15c1c19d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: TSK   FAT1 Owner identifier  0 Group identifier  0 Mode  0o000 Number of links  1\n",
            "\n",
            "\n",
            " {\n",
            "  \"Owner_identifier\": \"0\", \n",
            "  \"Group_identifier\": \"0\",\n",
            "  \"Mode\": \"0o000\",\n",
            "  \"Number_of_links\": \"1\"\n",
            "}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}